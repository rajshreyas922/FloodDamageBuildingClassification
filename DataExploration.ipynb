{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from random import randint\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from PIL import Image, ImageDraw\n",
    "import PIL\n",
    "from IPython.display import display\n",
    "from shapely import wkt\n",
    "from shapely.geometry.multipolygon import MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams\n",
    "from copy import copy \n",
    "\n",
    "%matplotlib inline\n",
    "# figure size in inches optional\n",
    "rcParams['figure.figsize'] = 30, 30\n",
    "plt.rcParams['legend.title_fontsize'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all post disaster json files\n",
    "labels_generator = Path('data/test/labels').rglob(pattern=f'*pre_*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group json files by disaster\n",
    "def get_disaster_dict(labels_generator):    \n",
    "    disaster_dict = defaultdict(list)\n",
    "    for label in labels_generator:\n",
    "        disaster_type = label.name.split('_')[0]\n",
    "        disaster_dict[disaster_type].append(str(label))\n",
    "    return disaster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_dict = get_disaster_dict(labels_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['guatemala-volcano', 'hurricane-florence', 'hurricane-harvey', 'hurricane-matthew', 'hurricane-michael', 'mexico-earthquake', 'midwest-flooding', 'palu-tsunami', 'santa-rosa-wildfire', 'socal-fire'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of Disasters present in the dataset\n",
    "disaster_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a particular disaster\n",
    "disaster_labels = disaster_dict['hurricane-matthew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(label_path):\n",
    "    with open(label_path) as json_file:\n",
    "        image_json = json.load(json_file)\n",
    "        return image_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color codes for polygons\n",
    "damage_dict = {\n",
    "    \"no-damage\": (255, 255, 255),\n",
    "    \"minor-damage\": (255, 255, 255),\n",
    "    \"major-damage\": (255, 255, 255),\n",
    "    \"destroyed\": (255, 255, 255),\n",
    "    \"un-classified\": (255, 255, 255)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damage to integer mapping\n",
    "damage_dict_multi = {\n",
    "    \"no-damage\": 1,\n",
    "    \"minor-damage\": 2,\n",
    "    \"major-damage\": 3,\n",
    "    \"destroyed\": 4,\n",
    "    \"un-classified\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_damage_type(properties):\n",
    "    if 'subtype' in properties:\n",
    "        return properties['subtype']\n",
    "    else:\n",
    "        return 'no-damage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_img(draw, coords):\n",
    "        wkt_polygons = []\n",
    "\n",
    "        for coord in coords:\n",
    "            damage = get_damage_type(coord['properties'])\n",
    "            wkt_polygons.append((damage, coord['wkt']))\n",
    "\n",
    "        polygons = []\n",
    "\n",
    "        for damage, swkt in wkt_polygons:\n",
    "            polygons.append((damage, wkt.loads(swkt)))\n",
    "\n",
    "        for damage, polygon in polygons:\n",
    "            x,y = polygon.exterior.coords.xy\n",
    "            coords = list(zip(x,y))\n",
    "            draw.polygon(coords, damage_dict[damage])\n",
    "\n",
    "        del draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_img_multi(draw, coords):\n",
    "    wkt_polygons = []\n",
    "\n",
    "    for coord in coords:\n",
    "        damage = get_damage_type(coord['properties'])\n",
    "        wkt_polygons.append((damage, coord['wkt']))\n",
    "\n",
    "    polygons = []\n",
    "\n",
    "    for damage, swkt in wkt_polygons:\n",
    "        polygons.append((damage, wkt.loads(swkt)))\n",
    "\n",
    "    for damage, polygon in polygons:\n",
    "        x, y = polygon.exterior.coords.xy\n",
    "        coords = list(zip(x, y))\n",
    "        \n",
    "        draw.polygon(coords, fill=damage_dict_multi[damage])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(json_path):\n",
    "    image_json = read_label(json_path)\n",
    "    img_name = image_json['metadata']['img_name']\n",
    "  \n",
    "    img = Image.new('RGB', (1024, 1024), color='black')\n",
    "    draw = ImageDraw.Draw(img, 'RGBA')\n",
    "    annotate_img(draw, image_json['features']['xy'])\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_multi(json_path):\n",
    "    image_json = read_label(json_path)\n",
    "    img_name = image_json['metadata']['img_name']\n",
    "\n",
    "\n",
    "\n",
    "    img = Image.new('L', (1024, 1024), color=0)  \n",
    "    draw = ImageDraw.Draw(img)\n",
    "    annotate_img_multi(draw, image_json['features']['xy'])\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296,)\n"
     ]
    }
   ],
   "source": [
    "# # Loop through each JSON path in disaster_labels\n",
    "labels_generator = Path('data\\\\test\\\\labels').rglob(pattern=f'*post*.json')\n",
    "disaster_dict = get_disaster_dict(labels_generator)\n",
    "\n",
    "c = 0\n",
    "flooding_subset = []\n",
    "for i in disaster_dict.keys():\n",
    "    disaster_labels = disaster_dict[i]\n",
    "    for j in disaster_labels:\n",
    "        disaster_type = read_label(j)['metadata']['disaster_type']\n",
    "        if disaster_type == \"flooding\":\n",
    "            img_path = j.replace('labels', 'images').replace('json', 'tif')\n",
    "            flooding_subset.append(img_path)\n",
    "flooding_subset = np.array(flooding_subset)\n",
    "print(flooding_subset.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\test\\images\\hurricane-florence_00000004_post_disaster.tif\n"
     ]
    }
   ],
   "source": [
    "print(flooding_subset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in flooding_subset:\n",
    "\n",
    "    image = tifffile.imread(str(path)) \n",
    "    base = path[17:]\n",
    "    path = 'data\\\\test_set\\images\\\\' + base[:-4] + '.png'\n",
    "    cv2.imwrite(path, image)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('data\\damage_classification_train\\multiclass_segmentation_labels\\\\'+base_name+'.npy').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\tier3\\labels\\nepal-flooding_00000000_post_disaster.json\n"
     ]
    }
   ],
   "source": [
    "labels_generator = Path('data/tier3/labels').rglob(pattern=f'*post_*.json')\n",
    "disaster_dict = get_disaster_dict(labels_generator)\n",
    "c = 0\n",
    "flooding_subset = []\n",
    "for i in disaster_dict.keys():\n",
    "    disaster_labels = disaster_dict[i]\n",
    "    for j in disaster_labels:\n",
    "        disaster_type = read_label(j)['metadata']['disaster_type']\n",
    "        if disaster_type == \"flooding\":\n",
    "            flooding_subset.append(j)\n",
    "print(flooding_subset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajsh\\AppData\\Local\\Temp\\ipykernel_19856\\3782621949.py:9: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tifffile.imsave(os.path.join('data/damage_classification_train/images', base_name + '.tif'), image)\n"
     ]
    }
   ],
   "source": [
    "for json_path in flooding_subset:\n",
    "    # Get corresponding image path\n",
    "    img_path = json_path.replace('labels', 'images').replace('json', 'tif')\n",
    "    # Read the image\n",
    "    image = tifffile.imread(img_path)\n",
    "    # Extract filename (without extension) to use for saving\n",
    "    base_name = os.path.basename(json_path).replace('.json', '')\n",
    "    # Save the image and mask\n",
    "    tifffile.imsave(os.path.join('data/damage_classification_train/images', base_name + '.tif'), image)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the JSON files to the destination directory\n",
    "for file_path in flooding_subset:\n",
    "    # Read the JSON content from the original file\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_content = json.load(file)\n",
    "    \n",
    "    # Define the new path where the JSON file will be saved\n",
    "    new_file_path = 'data\\\\damage_classification_train\\\\json_labels\\\\' + Path(file_path).name\n",
    "\n",
    "\n",
    "    # Write the JSON content to the new file\n",
    "    with open(new_file_path, 'w') as file:\n",
    "        json.dump(json_content, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55465 8078 14323 1032 2583\n",
      "0.6807108405640578 0.09913967673445342 0.17578331144684037 0.012665529387219106 0.03170064186742921\n"
     ]
    }
   ],
   "source": [
    "import tifffile\n",
    "from shapely.wkt import loads\n",
    "from PIL import Image, ImageDraw\n",
    "labels_generator = Path('data/damage_classification_train/json_labels').rglob(pattern=f'*post_*.json')\n",
    "disaster_dict = get_disaster_dict(labels_generator)\n",
    "\n",
    "def get_bounding_box(wkt_polygon, margin=5):\n",
    "    \"\"\"\n",
    "    Calculate the bounding box for a given polygon and expand it by the specified margin.\n",
    "    :param wkt_polygon: WKT representation of the polygon.\n",
    "    :param margin: Amount by which to expand the bounding box on each side.\n",
    "    :return: Coordinates of the expanded bounding box.\n",
    "    \"\"\"\n",
    "    polygon = loads(wkt_polygon)\n",
    "\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    center_x = (minx + maxx) / 2\n",
    "    center_y = (miny + maxy) / 2\n",
    "    minx = center_x - 75\n",
    "    maxx = center_x + 75\n",
    "    miny = center_y - 75\n",
    "    maxy = center_y + 75\n",
    "    # Get the Center of the the box. Return b\n",
    "    # Expand the bounding box\n",
    "    return [(minx, miny), (maxx, maxy)]\n",
    "\n",
    "def create_directory(path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def save_cropped_image(image, bbox, save_path):\n",
    "    \"\"\"Crop and save the image based on the bounding box.\"\"\"\n",
    "    cropped_image = image.crop((bbox[0][0], bbox[0][1], bbox[1][0], bbox[1][1]))\n",
    "    cropped_image.save(save_path)\n",
    "\n",
    "# Create directories for each damage type\n",
    "base_dir = 'data/damage_classification_train/extracted_buildings'\n",
    "nd = 0\n",
    "mind = 0\n",
    "majd = 0\n",
    "destr = 0\n",
    "unc = 0\n",
    "# Process each image and crop the buildings\n",
    "for i in disaster_dict.keys():\n",
    "    disaster_labels = disaster_dict[i]\n",
    "    for j in disaster_labels:\n",
    "        disaster = read_label(j)\n",
    "        img_path = j.replace('json_labels', 'images').replace('json', 'tif')\n",
    "        image = tifffile.imread(img_path) \n",
    "        image_pil = Image.fromarray(np.array(image, dtype=np.uint8))  \n",
    "        \n",
    "        wkt_polygons = []\n",
    "        for coord in disaster['features']['xy']:\n",
    "            damage = get_damage_type(coord['properties'])\n",
    "            wkt_polygons.append((damage, coord['wkt']))\n",
    "        k = 0\n",
    "        for damage, wkt_polygon in wkt_polygons:\n",
    "            bbox = get_bounding_box(wkt_polygon)\n",
    "            # Determine the save path based on damage type\n",
    "            if damage == 'no-damage':\n",
    "                save_path = os.path.join(base_dir, '0')\n",
    "                nd = nd + 1\n",
    "            elif damage == 'minor-damage':\n",
    "                save_path = os.path.join(base_dir, '1')\n",
    "                mind += 1\n",
    "            elif damage == 'major-damage':\n",
    "                save_path = os.path.join(base_dir, '2')\n",
    "                majd += 1\n",
    "            elif damage == 'destroyed':\n",
    "                save_path = os.path.join(base_dir, '3')\n",
    "                destr += 1\n",
    "            else:\n",
    "                save_path = os.path.join(base_dir, 'unclassified')\n",
    "                unc += 1\n",
    "        \n",
    "            # Generate a unique filename for each cropped image\n",
    "            filename = f\"{i}_{os.path.basename(j).split('.')[0]}_{damage}\"\n",
    "            filename = filename + '_'+str(k) + '.png'\n",
    "            k += 1\n",
    "            save_cropped_image(image_pil, bbox, os.path.join(save_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55465 8078 14323 1032 2583\n",
      "0.6807108405640578 0.09913967673445342 0.17578331144684037 0.012665529387219106 0.03170064186742921\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(nd, mind, majd, destr, unc)\n",
    "total = nd + mind + majd + destr + unc\n",
    "print(nd/total, mind/total, majd/total, destr/total, unc/total)\n",
    "print(nd/total+ mind/total+ majd/total+ destr/total+ unc/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from shapely.wkt import loads\n",
    "from PIL import Image\n",
    "\n",
    "def create_directory(path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def save_cropped_image(image, bbox, save_path):\n",
    "    \"\"\"Crop and save the image based on the bounding box.\"\"\"\n",
    "    cropped_image = image.crop((bbox[0][0], bbox[0][1], bbox[1][0], bbox[1][1]))\n",
    "    cropped_image.save(save_path)\n",
    "\n",
    "# Create directories for each damage type\n",
    "base_dir = 'data/damage_classification_train/extracted_buildings'\n",
    "damage_dirs = ['0', '1', '2', 'unclassified']\n",
    "for damage_dir in damage_dirs:\n",
    "    create_directory(os.path.join(base_dir, damage_dir))\n",
    "\n",
    "# Process each image and crop the buildings\n",
    "for i in disaster_dict.keys():\n",
    "    disaster_labels = disaster_dict[i]\n",
    "    for j in disaster_labels:\n",
    "        disaster = read_label(j)\n",
    "\n",
    "        img_path = j.replace('json_labels', 'images').replace('json', 'tif')\n",
    "        image = tifffile.imread(img_path) \n",
    "        image_pil = Image.fromarray(np.array(image, dtype=np.uint8))  \n",
    "        \n",
    "        wkt_polygons = []\n",
    "        for coord in disaster['features']['xy']:\n",
    "            damage = get_damage_type(coord['properties'])\n",
    "            wkt_polygons.append((damage, coord['wkt']))\n",
    "\n",
    "        for damage, wkt_polygon in wkt_polygons:\n",
    "            bbox = get_bounding_box(wkt_polygon)\n",
    "            # Determine the save path based on damage type\n",
    "            if damage == 'no-damage':\n",
    "                save_path = os.path.join(base_dir, '0')\n",
    "            elif damage == 'minor-damage':\n",
    "                save_path = os.path.join(base_dir, '1')\n",
    "            elif damage == 'major-damage':\n",
    "                save_path = os.path.join(base_dir, '2')\n",
    "            else:\n",
    "                save_path = os.path.join(base_dir, 'unclassified')\n",
    "            \n",
    "            # Generate a unique filename for each cropped image\n",
    "            filename = f\"{i}_{os.path.basename(j).split('.')[0]}_{damage}.png\"\n",
    "            save_cropped_image(image_pil, bbox, os.path.join(save_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\xBD\\DataExploration.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/xBD/DataExploration.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Loop through each JSON path in disaster_labels\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/xBD/DataExploration.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m labels_generator \u001b[39m=\u001b[39m Path(\u001b[39m'\u001b[39m\u001b[39mdata/tier1/labels\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mrglob(pattern\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*pre_*.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/xBD/DataExploration.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m disaster_dict \u001b[39m=\u001b[39m get_disaster_dict(labels_generator)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/xBD/DataExploration.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m c \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop through each JSON path in disaster_labels\n",
    "labels_generator = Path('data/tier1/labels').rglob(pattern=f'*pre_*.json')\n",
    "disaster_dict = get_disaster_dict(labels_generator)\n",
    "c = 0\n",
    "flooding_subset = []\n",
    "for i in disaster_dict.keys():\n",
    "    disaster_labels = disaster_dict[i]\n",
    "    for j in disaster_labels:\n",
    "        disaster_type = read_label(j)['metadata']['disaster_type']\n",
    "        if disaster_type == \"flooding\":\n",
    "            flooding_subset.append(j)\n",
    "\n",
    "labels_generator = Path('data/tier3/labels').rglob(pattern=f'*pre_*.json')\n",
    "disaster_dict = get_disaster_dict(labels_generator)\n",
    "for i in disaster_dict.keys():\n",
    "    disaster_labels = disaster_dict[i]\n",
    "    for j in disaster_labels:\n",
    "        disaster_type = read_label(j)['metadata']['disaster_type']\n",
    "        if disaster_type == \"flooding\":\n",
    "            flooding_subset.append(j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\damage_classification_train\\images\\hurricane-florence_00000389_post_disaster.tif\n"
     ]
    }
   ],
   "source": [
    "json_path = flooding_subset[13]\n",
    "print(json_path)\n",
    "img_path = json_path.replace('labels', 'images').replace('json', 'tif').replace('post', 'pre')\n",
    "img_path = 'data\\damage_classification_train\\images\\hurricane-florence_00000389_post_disaster.tif'\n",
    "print(img_path)\n",
    "    # Read the image\n",
    "image = tifffile.imread(img_path)/255.0\n",
    "image = cv2.resize(image,(572,572))\n",
    "cv2.imshow(\"sample\", image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "# Get the mask using your function\n",
    "mask = get_mask(json_path)\n",
    "mask.show()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure folders exist\n",
    "if not os.path.exists('data/train/images'):\n",
    "    os.makedirs('data/train/images')\n",
    "if not os.path.exists('data/train/masks'):\n",
    "    os.makedirs('data/train/masks')\n",
    "\n",
    "\n",
    "for json_path in flooding_subset:\n",
    "    # Get corresponding image path\n",
    "    img_path = json_path.replace('labels', 'images').replace('json', 'tif')\n",
    "    # Read the image\n",
    "    image = tifffile.imread(img_path)\n",
    "    \n",
    "    # Get the mask using your function\n",
    "    mask = get_mask(json_path)\n",
    "    \n",
    "    # Extract filename (without extension) to use for saving\n",
    "    base_name = os.path.basename(json_path).replace('.json', '')\n",
    "    \n",
    "    # Save the image and mask\n",
    "    tifffile.imsave(os.path.join('data/train/images', base_name + '.tif'), image)\n",
    "    \n",
    "    # Assuming the mask is also in a format compatible with tifffile, save it\n",
    "    # Modify as necessary depending on the mask format\n",
    "    tifffile.imsave(os.path.join('data/train/masks', base_name + '_mask.tif'), mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "(1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(\"data\\\\train\\\\masks\")))\n",
    "mask = tifffile.imread(\"data\\\\train\\\\masks\\\\hurricane-florence_00000000_pre_disaster_mask.tif\")\n",
    "print(mask.shape)\n",
    "# Noise Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_third_underscore(s):\n",
    "    parts = s.split('_')\n",
    "    if len(parts) < 3:\n",
    "        # If there are less than 4 parts, there's no third underscore to replace\n",
    "        return s\n",
    "    return '_'.join(parts[:3]) + '-' + '_'.join(parts[3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def replace_third_underscore(s):\n",
    "    parts = s.split('_')\n",
    "    if len(parts) < 4:\n",
    "        return s\n",
    "    return '_'.join(parts[:3]) + '-' + '_'.join(parts[3:])\n",
    "\n",
    "target = 'final\\\\results'  # Directory with the original files\n",
    "\n",
    "# Iterate over each file in the target directory\n",
    "for path in os.listdir(target):\n",
    "    # Construct the full path\n",
    "    full_path = os.path.join(target, path)\n",
    "    \n",
    "    # Skip directories, only rename files\n",
    "    if os.path.isfile(full_path):\n",
    "        new_name = replace_third_underscore(path)\n",
    "        new_full_path = os.path.join(target, new_name)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(full_path, new_full_path)\n",
    "        print(f\"Renamed '{path}' to '{new_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Normalization and replacement of images complete.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the directories\n",
    "target_dir = Path('final/targets')\n",
    "results_dir = Path('final/results')\n",
    "\n",
    "\n",
    "# Function to normalize and save images\n",
    "def normalize_and_replace_images(directory):\n",
    "    for file in directory.iterdir():\n",
    "        # Check if the file is an image and contains 'localization' in its name\n",
    "        if file.suffix == '.png' and 'localization' in file.stem:\n",
    "            # Read the image in grayscale\n",
    "            img = cv2.imread(str(file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # Normalize the image to the range [0, 1]\n",
    "            normalized_img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "            # Replace the original image with the normalized image\n",
    "            cv2.imwrite(str(file), normalized_img)\n",
    "\n",
    "# Normalize and replace images in both directories\n",
    "normalize_and_replace_images(target_dir)\n",
    "normalize_and_replace_images(results_dir)\n",
    "\n",
    "\"Normalization and replacement of images complete.\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
